This project predicts housing prices using the **California Housing dataset** instead of the **Boston Housing dataset** due to its removal from `scikit-learn`. It employs **Linear Regression and Ridge Regression** to analyze key factors affecting house prices.

# Import libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Step 1: Load the dataset
california = fetch_california_housing()
data = pd.DataFrame(california.data, columns=california.feature_names)
target = pd.Series(california.target, name="Price")

# Combine the features and target into one DataFrame
california_df = pd.concat([data, target], axis=1)

# Step 2: Data Exploration
print("Dataset Overview:")
print(california_df.info())
print("\nFirst few rows of the dataset:")
print(california_df.head())

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(california_df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Step 3: Data Preprocessing
# Normalize or standardize the data
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(data), columns=california.feature_names)
y = target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Model Building 
# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)  # Regularization strength
ridge_model.fit(X_train, y_train)

# Step 5: Model Evaluation
# Predictions and evaluate effectiveness of Ridge Regression over Linear Regression when overfitting occurs.

linear_pred = linear_model.predict(X_test)
ridge_pred = ridge_model.predict(X_test)

# Evaluation metrics
linear_mse = mean_squared_error(y_test, linear_pred)
linear_r2 = r2_score(y_test, linear_pred)
ridge_mse = mean_squared_error(y_test, ridge_pred)
ridge_r2 = r2_score(y_test, ridge_pred)

# Display results of liner and ridge regression 
print("\nLinear Regression Performance:")
print(f"Mean Squared Error: {linear_mse:.2f}")
print(f"R-squared Score: {linear_r2:.2f}")

print("\nRidge Regression Performance:")
print(f"Mean Squared Error: {ridge_mse:.2f}")
print(f"R-squared Score: {ridge_r2:.2f}")

# Step 6: Visualizations
# Predictions vs Actual
plt.figure(figsize=(10, 6))
plt.scatter(y_test, linear_pred, label="Linear Regression", alpha=0.7)
plt.scatter(y_test, ridge_pred, label="Ridge Regression", alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label="Ideal Fit")
plt.title("Predicted vs Actual Prices")
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.legend()
plt.show()

# Residuals distribution
linear_residuals = y_test - linear_pred
ridge_residuals = y_test - ridge_pred

plt.figure(figsize=(12, 6))
sns.histplot(linear_residuals, kde=True, color='blue', label="Linear Regression", alpha=0.6)
sns.histplot(ridge_residuals, kde=True, color='green', label="Ridge Regression", alpha=0.6)
plt.title("Residuals Distribution")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.legend()
plt.show()
